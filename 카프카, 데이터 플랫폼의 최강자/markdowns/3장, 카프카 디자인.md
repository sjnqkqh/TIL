# 3장, 카프카 디자인

#### 3장, 카프카 디자인

***

* 카프카는 애초부터 실시간 로그 집계 같은 대용량 데이터 처리를 염두에 두고 만들어졌으며, 배치 전송, 파티션, 분산 기능을 구현해두었음
* 기존 카프카 제작사 링크드인은 메시징 큐 시스템으로 아파치 액티브엠큐를 사용했는데, 불편이 많아서 데이터 파이프라인을 통합하고, 처리량이 높은 시스템을 개발하기로 함.

\


#### 분산 시스템

***

* 같은 역할을 하는 여러 대의 컴퓨터가 하나의 그룹으로 이뤄지는 형태

1. 단일 시스템보다 더 높은 성능을 얻을 수 있고
2. 분산 시스템 중 일부 노드에 장애가 발생하면 다른 노드가 대신 처리하며
3. 수평적 확장에 용이하다

\


#### 페이지 캐시

***

* 운영체제는 물리적 메모리에 애플리케이션이 사용하고 남는 메모리 중 일부를 페이지 캐시로 유지해 OS의 성능 향상을 가져오는데, 카프카는 이 페이지 캐시를 이용하도록 디자인 되어있음
* 때문에 카프카를 구성할 땐 디스크 중 가격이 가장 저렴한 SATA를 사용해도 무방함.
* 컨플루언트 사의 도큐먼트엔 카프카를 다룰 때 5GB 정도의 메모리면 충분하고, 남는 메모리는 페이지 캐시로 다루길 권한다. 또한 페이지 캐시를 서로 공유해야하기 때문에 하나의 시스템에 카프카와 다른 애플리케이션을 함께 실행하는건 권장되지 않는다.

\


#### 배치 전송 처리

***

![Untitled](assets/0305fce1\_Untitled.png)

\


### 카프카 데이터 모델

#### 토픽

***

* 데이터 저장소. 일종의 DB 테이블처럼 사용한다.
* 여러 서비스에서 하나의 클러스터를 공유해서 사용할 수도 있으니, 되도록 명명 규칙도 확장성 있는 것이 좋다. 간단하게는 prefix를 사용하는 것만으로도 좋다.

\


#### 파티션

***

* 토픽을 분할한 것
* 토픽에 데이터를 저장할 때 파티션을 사용하면 각 데이터를 분할하여 저장할 수 있다.
* 또한 메시지 큐 시스템은 항상 메시지의 순서가 보장되어야 하기에 프로듀서를 늘리는 것만으로는 다른 애플리케이션처럼 성능 향상이 일어나지 않으므로 여러개의 파티션이 필요함.
* 그렇다고 무조건 파티션이 많다고 좋은건 아닌게

![Untitled](assets/ba9908d8\_Untitled.png)

![Untitled](assets/e01d82ff\_Untitled.png)

![Untitled](assets/02320588\_Untitled.png)

***

\


1. 각 파티션은 브로커 디렉토리와 매핑되고 저장되는 데이터마다 인덱스 파일과 실제 데이터를 저장한다. 카프카에선 모든 디렉토리 파일에 대해 파일 핸들을 열게 되고, 따라서 파티션의 수가 많을 수록 파일 핸들 수가 증가하기에 리소스가 낭비된다.
2. 카프카는 리플리케이션을 지원하는데 장애 복구 시 각 파티션마다 리플리케이션이 동작하게 되고, 파티션마다 하나의 리더를 갖는다. 만약 브로커 하나가 다운된다면 해당 브로커의 파티션을 제외한 다른 브로커의 파티션 중 리더를 선출하는데, 이 때 파티션이 과도하게 많으면 리더를 선출하는데 걸리는 시간이 선형으로 증가한다.
3. 만약 최악의 경우 다운된 브로커가 카프카 컨트롤러인 경우, 정상 브로커 중 하나로 컨트롤러를 대체하기 전까지 새로운 리더를 선출할 수 없으므로 주키퍼에서 모든 파티션의 데이터를 읽어야한다. 이 경우에도 파티션이 과도하게 많다면 긴 장애시간을 소요하게 된다.

\


#### 토픽에 따른 적정 파티션 수

***

* 원하는 목표 초당 처리량 기준을 잡아야한다.
* 프로듀서들에서 생산되는 1초당 생산량과 근사치의 처리량을 갖는 파티션 갯수가 있어야 하고
* 컨슈머가 더 많다면, 컨슈머의 수에 맞춰서 파티셔닝을 하는 것이 좋다.
* 어쨋든 중요한건 예상 목표치를 가지고 파티션을 할당하는 것
* 다만 중요한건 파티션의 수를 늘리는 것은 자유롭지만, 파티션의 수를 줄이는 방법은 제공하지 않는다. 과하게 파티션을 늘리면, 이걸 줄일 수 있는 방법을 토픽을 삭제하고 동일한 토픽을 다시 만드는 것 뿐이다.
* 때문에 적절한 수의 파티션을 계산하기 어렵다면 일단 적은 수의 파티션으로 운영하고, 조금씩 파티션과 프로듀서, 컨슈머를 늘려가는 편이 좋다.
* 카프카에선 브로커 당 약 2000개를 최대 파티션 수로 권장한다.

\


#### 오프셋과 메시지 순서

***

* 오프셋은, 각 파티션마다 메시지가 저장되는 위치
*   오프셋은 파티션에에서 유일하고 순차적으로 증가하는 64비트 정수로 되어있다.

    ![Untitled](assets/be382449\_Untitled.png)
* 컨슈머가 파티션 0에서 데이터를 가져간다고 하면 반드시 0,1,2,3… 순서로 가져가야하며, 변경은 절대로 불가능하다.

\


#### 리플리케이션 팩터, 리더, 팔로워

***

* 토픽 생성 시 마다 지정하거나, 디폴트 값을 설정할 수 있음.
* 리플리케이션 팩터를 증가시키면, 하나의 파티션이 저장되는 브로커의 수를 증가시킨다. 고로 하나의 브로커가 다운되더라도 다른 브로커에서 해당 토픽을 처리할 수 있다. (고가용성)
* 이 중 원본과 복제본을 구분하기 위한 용어는 서비스마다 다른데 주키퍼에선 리더와 팔로워, 레빗엠큐에선 마스터큐,미러드큐 라고 불린다. 카프카에선 주키퍼와 동일하게 리더, 팔로워
* **중요한 점은 모든 읽기, 쓰기는 리더를 통해서만 일어남**
* 팔로워는 저장된 오프셋과 데이터만 복사해둠
* 리더로 지정된 브로커가 다운된다면 팔로워 중 하나가 새로운 리더 브로커가 된다. 데이터는 동일하게 저장되고 있었으니, 프로듀싱에 별 문제는 없다
* 다만, 리플리케이션 팩터의 수와 동일하게 저장 용량이 선형으로 증가한다.
* 또한 브로커의 리소스 사용량이 증가한다. 브로커에선 지속적으로 리플리케이션이 잘 작동하는지 유효성을 검사하므로
* 따라서 무조건 리플리케이션 팩터를 증가시키기보단, 중요도에 따라 리플리케이션 팩터를 2,3으로 늘려서 운영하는 것이 더 바람직하다
* 팔로워는 주기적으로 리더의 데이터를 폴링하여 관리하는데, 데이터가 정확히 일치하지 않는 상황에서 리더가 다운되면 정합성 문제가 발생함.
* 이를 방지하기 위해 `ISR`라는 그룹을 만들어 신뢰성을 높인다.
* ISR란, In Sync Replica 의 약어로 리플리케이션 팩터 중 리더가 될 수 있는 구성원들을 의미한다.
* 팔로워들은 매우 짧은 주기 동안, 리더의 데이터의 변경 사항을 감지하는데 이런 요청이 일정시간 끊기게 된다면 리더는 해당 팔로워의 이상을 감지하고 ISR에서 방출한다.

\


#### 만약 모든 브로커가 다운된다면…

***

![Untitled](assets/20ca4e57\_Untitled.png)

이게 둠스데이지…

이 경우 사용자에게 주어진 선택지는

1. 마지막 리더가 살아나기를 기도한다.
2. ISR에서 추방되었지만 먼저 살아나면 자동으로 리더가 된다.

\


1→ 최악의 상황에서 그나마 나은 상황. 왜냐면 마지막까지 살아남은 리더 브로커가 살아났기에, 프로듀싱에 실패한 D를 제외하고는 메시지 손실이 전혀 발생하지 않기 때문

→ 다만 리더 브로커가 알 수 없는 이유로 살아나지 않는다면 절망적인 상황이 된다. 리더가 정상화 될 때까지 장애 상황이 지속된다.

\


2→ 가장 빠르게 장애 상황을 해소할 수 있지만, ISR에서 제외된 브로커기에 메시지 손실이 발생함. 나중에 살아난 기존 리더 브로커는 새로운 리더가 선출되었으므로 팔로워 브로커로 강등되어 새 브로커의 메시지를 동기화 함. 결국 장애 상황이 종료되고 메시지 손실이 발생한 것

\


* 결국 이 문제에선 **실버 불릿이 존재하지 않고, 가용성과 데이터 일관성 사이에서 선택이 필요한 케이스**다.
* 다만 카프카 0.11.0.0 버전 이후에선 데이터 손실을 감안하는 2번 케이스를 기본값으로 설정한다. 물론 변경 가능

\


#### 카프카에서 사용하는 주키퍼 지노드 역할

***

![Untitled](assets/3b876a36\_Untitled.png)

\


주키퍼 지노드는 총 2 종류로,

1. **영구 노드**: Delete를 호출하여 지울 수 있음
2. **임시 노드**: 생성한 클라이언트의 연결이 끊어지거나 장애가 발생하면 삭제된다.

\


* /controller - 카프카 클러스터의 컨트롤러 정보. 카프카에선 클러스터 내의 브로커 중 하나를 컨트롤러로 선정하여 브로커 레벨에서 실패를 감지하고 실패한 브로커에 의해 영향받는 모든 파티션의 리더 변경을 책임진다.

클러스터 내 브로커 중 임의로 선정되고, 컨트롤러 브로커가 다운되면 남아있는 브로커 중 하나가 새로운 컨트롤러가 된다.

* /brokers - 브로커 관련 정보를 저장한다. 주키퍼의 임시 노드를 사용해 등록하고, 브로커가 종료되거나 다운되면 지노드가 사라진다.
* /consumers - 컨슈머 관련 정보를 저장하며 컨슈머가 각 파티션들에 대해 어디까지 읽었는지 기록하는 오프셋 정보가 여기 담긴다. 이 정보들은 사라지면 안 되기 때문에 영구 노드로 저장된다.

구버전에선 컨슈머 오프셋를 주키퍼 혹은 카프카 토픽으로 저장했지만, 최신 버전에선 카프카 토픽에 저장한다. 주키퍼는 Deprecated됨.

* /config - 토픽의 상세 설정 정보를 확인할 수 있다.

\
